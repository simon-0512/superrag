"""
APIè·¯ç”±
"""

from flask import Blueprint, request, jsonify, current_app, Response, stream_with_context
from flask_login import login_required, current_user
import uuid
from datetime import datetime, timedelta
import logging
import json
import threading
import time
from queue import Queue
import traceback

from app.models import User, KnowledgeBase, Conversation, Message
from app.database import db
from app.services import DeepSeekService
from app.services.conversation_service import ConversationService

logger = logging.getLogger(__name__)

# åˆ›å»ºAPIè“å›¾
api_bp = Blueprint('api', __name__, url_prefix='/api')

# DeepSeekæœåŠ¡å°†åœ¨ä½¿ç”¨æ—¶åˆå§‹åŒ–
deepseek_service = None

# æ¶ˆæ¯ä¿å­˜é˜Ÿåˆ—
message_save_queue = Queue()
message_save_thread = None

def get_deepseek_service():
    """è·å–DeepSeekæœåŠ¡å®ä¾‹"""
    global deepseek_service
    if deepseek_service is None:
        deepseek_service = DeepSeekService()
    return deepseek_service

def save_message_worker():
    """ç‹¬ç«‹çº¿ç¨‹å·¥ä½œå™¨ï¼Œä¸“é—¨å¤„ç†æ¶ˆæ¯ä¿å­˜"""
    import queue
    from app import create_app
    
    # åˆ›å»ºç‹¬ç«‹çš„åº”ç”¨å®ä¾‹
    app = create_app()
    
    logger.info("ğŸ”§ æ¶ˆæ¯ä¿å­˜å·¥ä½œçº¿ç¨‹å·²å¯åŠ¨ï¼Œç­‰å¾…æ¶ˆæ¯...")
    
    while True:
        try:
            # ä»é˜Ÿåˆ—è·å–å¾…ä¿å­˜çš„æ¶ˆæ¯æ•°æ®
            save_data = message_save_queue.get(timeout=30)  # 30ç§’è¶…æ—¶
            
            if save_data is None:  # åœæ­¢ä¿¡å·
                logger.info("ğŸ”š æ”¶åˆ°åœæ­¢ä¿¡å·ï¼Œæ¶ˆæ¯ä¿å­˜å·¥ä½œçº¿ç¨‹é€€å‡º")
                break
                
            conversation_id = save_data['conversation_id']
            ai_msg_id = save_data['ai_msg_id']
            ai_response = save_data['ai_response']
            debug_info = save_data['debug_info']
            user_id = save_data['user_id']
            retry_count = save_data.get('retry_count', 0)
            
            debug_info['db_operations'] = debug_info.get('db_operations', [])
            
            try:
                # ä½¿ç”¨ç‹¬ç«‹çš„åº”ç”¨ä¸Šä¸‹æ–‡
                with app.app_context():
                    # è®°å½•æ•°æ®åº“æ“ä½œå¼€å§‹
                    operation_start = time.time()
                    debug_info['db_operations'].append({
                        'operation': 'save_ai_message_start',
                        'timestamp': operation_start,
                        'thread_id': threading.current_thread().ident,
                        'ai_msg_id': ai_msg_id,
                        'conversation_id': conversation_id,
                        'response_length': len(ai_response),
                        'retry_count': retry_count
                    })
                    
                    # éªŒè¯å¯¹è¯æ˜¯å¦å­˜åœ¨
                    conversation = db.session.query(Conversation).filter_by(
                        id=conversation_id,
                        user_id=user_id
                    ).first()
                    
                    if not conversation:
                        debug_info['db_operations'].append({
                            'operation': 'conversation_not_found',
                            'timestamp': time.time(),
                            'conversation_id': conversation_id,
                            'error': 'å¯¹è¯ä¸å­˜åœ¨'
                        })
                        logger.error(f"ä¿å­˜æ¶ˆæ¯å¤±è´¥ï¼šå¯¹è¯ {conversation_id} ä¸å­˜åœ¨")
                        message_save_queue.task_done()
                        continue
                    
                    # æ£€æŸ¥æ¶ˆæ¯æ˜¯å¦å·²å­˜åœ¨ï¼ˆé˜²æ­¢é‡å¤ä¿å­˜ï¼‰
                    existing_msg = db.session.query(Message).filter_by(id=ai_msg_id).first()
                    if existing_msg:
                        debug_info['db_operations'].append({
                            'operation': 'message_already_exists',
                            'timestamp': time.time(),
                            'ai_msg_id': ai_msg_id,
                            'existing_content_length': len(existing_msg.content)
                        })
                        logger.info(f"æ¶ˆæ¯ {ai_msg_id} å·²å­˜åœ¨ï¼Œè·³è¿‡ä¿å­˜")
                        message_save_queue.task_done()
                        continue
                    
                    # ä¿å­˜AIæ¶ˆæ¯
                    ai_msg = Message(
                        id=ai_msg_id,
                        conversation_id=conversation_id,
                        role='assistant',
                        content=ai_response,
                        token_count=len(ai_response.split()),  # ç®€å•çš„tokenè®¡ç®—
                        created_at=datetime.utcnow()
                    )
                    db.session.add(ai_msg)
                    
                    # æ›´æ–°å¯¹è¯ä¿¡æ¯
                    conversation.message_count += 1
                    conversation.updated_at = datetime.utcnow()
                    
                    # æäº¤äº‹åŠ¡
                    db.session.commit()
                    
                    operation_end = time.time()
                    debug_info['db_operations'].append({
                        'operation': 'save_ai_message_success',
                        'timestamp': operation_end,
                        'duration': operation_end - operation_start,
                        'ai_msg_id': ai_msg_id,
                        'conversation_id': conversation_id,
                        'new_message_count': conversation.message_count,
                        'commit_successful': True
                    })
                    
                    logger.info(f"âœ… AIæ¶ˆæ¯å·²æˆåŠŸä¿å­˜: {ai_msg_id}, å¯¹è¯: {conversation_id}, è€—æ—¶: {operation_end - operation_start:.3f}s")
                    
                    # ğŸ”¥ é‡è¦ï¼šé€šçŸ¥LangChainæœåŠ¡åˆ·æ–°å†å²è®°å½•
                    try:
                        from app.services.conversation_service import ConversationService
                        conv_service = ConversationService()
                        if conv_service.langchain_enabled and conv_service.langchain_service:
                            # æ¸…é™¤è¯¥å¯¹è¯çš„å†å²ç¼“å­˜ï¼Œå¼ºåˆ¶é‡æ–°åŠ è½½
                            if conversation_id in conv_service.langchain_service._session_histories:
                                conv_service.langchain_service._session_histories[conversation_id].reload_from_database()
                                debug_info['db_operations'].append({
                                    'operation': 'langchain_history_refreshed',
                                    'timestamp': time.time(),
                                    'conversation_id': conversation_id
                                })
                                logger.info(f"ğŸ”„ LangChainå†å²è®°å½•å·²åˆ·æ–°: {conversation_id}")
                    except Exception as refresh_error:
                        debug_info['db_operations'].append({
                            'operation': 'langchain_refresh_error',
                            'timestamp': time.time(),
                            'error': str(refresh_error)
                        })
                        logger.warning(f"LangChainå†å²åˆ·æ–°å¤±è´¥: {str(refresh_error)}")
                    
            except Exception as save_error:
                error_time = time.time()
                debug_info['db_operations'].append({
                    'operation': 'save_ai_message_error',
                    'timestamp': error_time,
                    'error': str(save_error),
                    'error_type': type(save_error).__name__,
                    'traceback': traceback.format_exc(),
                    'retry_count': retry_count
                })
                
                logger.error(f"âŒ ä¿å­˜AIæ¶ˆæ¯å¤±è´¥ (å°è¯• {retry_count + 1}): {str(save_error)}")
                
                # å›æ»šæ•°æ®åº“äº‹åŠ¡
                try:
                    with app.app_context():
                        db.session.rollback()
                except Exception as rollback_error:
                    debug_info['db_operations'].append({
                        'operation': 'rollback_error',
                        'timestamp': time.time(),
                        'error': str(rollback_error)
                    })
                
                # é‡è¯•æœºåˆ¶ï¼ˆæœ€å¤šé‡è¯•3æ¬¡ï¼‰
                if retry_count < 3:
                    save_data['retry_count'] = retry_count + 1
                    debug_info['db_operations'].append({
                        'operation': 'retry_scheduled',
                        'timestamp': time.time(),
                        'retry_count': retry_count + 1,
                        'delay': 2 ** retry_count  # æŒ‡æ•°é€€é¿
                    })
                    
                    # å»¶è¿Ÿåé‡æ–°åŠ å…¥é˜Ÿåˆ—
                    def retry_save():
                        time.sleep(2 ** retry_count)  # æŒ‡æ•°é€€é¿ï¼š1s, 2s, 4s
                        message_save_queue.put(save_data)
                    
                    threading.Thread(target=retry_save, daemon=True).start()
                else:
                    debug_info['db_operations'].append({
                        'operation': 'save_failed_final',
                        'timestamp': time.time(),
                        'error': 'Max retries exceeded',
                        'final_error': str(save_error)
                    })
                    logger.error(f"ğŸ’€ AIæ¶ˆæ¯ä¿å­˜å½»åº•å¤±è´¥ï¼Œå·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°: {ai_msg_id}")
            
            message_save_queue.task_done()
            
        except queue.Empty:
            # é˜Ÿåˆ—è¶…æ—¶æ˜¯æ­£å¸¸æƒ…å†µï¼Œä¸æ˜¯å¼‚å¸¸ï¼Œä¸éœ€è¦è®°å½•é”™è¯¯æ—¥å¿—
            # logger.debug("â° æ¶ˆæ¯ä¿å­˜é˜Ÿåˆ—è¶…æ—¶ï¼Œç»§ç»­ç­‰å¾…...")
            continue
        except Exception as worker_error:
            # åªæœ‰çœŸæ­£çš„å¼‚å¸¸æ‰è®°å½•é”™è¯¯æ—¥å¿—
            logger.error(f"ğŸ’¥ æ¶ˆæ¯ä¿å­˜å·¥ä½œçº¿ç¨‹çœŸå®å¼‚å¸¸: {str(worker_error)}")
            logger.error(f"å¼‚å¸¸è¯¦æƒ…: {traceback.format_exc()}")
            time.sleep(1)  # é¿å…å¾ªç¯é”™è¯¯

def start_message_save_worker():
    """å¯åŠ¨æ¶ˆæ¯ä¿å­˜å·¥ä½œçº¿ç¨‹"""
    global message_save_thread
    if message_save_thread is None or not message_save_thread.is_alive():
        message_save_thread = threading.Thread(target=save_message_worker, daemon=True)
        message_save_thread.start()
        logger.info("âœ… æ¶ˆæ¯ä¿å­˜å·¥ä½œçº¿ç¨‹å·²å¯åŠ¨")

def cleanup_old_conversations(user_id, max_conversations=10):
    """æ¸…ç†ç”¨æˆ·çš„æ—§å¯¹è¯ï¼Œåªä¿ç•™æœ€è¿‘çš„max_conversationsç»„å¯¹è¯"""
    try:
        # è·å–ç”¨æˆ·çš„æ‰€æœ‰å¯¹è¯ï¼ŒæŒ‰æ›´æ–°æ—¶é—´å€’åºæ’åˆ—
        conversations = db.session.query(Conversation)\
            .filter_by(user_id=user_id, is_active=True)\
            .order_by(Conversation.updated_at.desc())\
            .all()
        
        # å¦‚æœè¶…è¿‡é™åˆ¶æ•°é‡ï¼Œåˆ é™¤æ—§çš„å¯¹è¯
        if len(conversations) > max_conversations:
            conversations_to_delete = conversations[max_conversations:]
            
            for conv in conversations_to_delete:
                logger.info(f"åˆ é™¤æ—§å¯¹è¯: {conv.id} - {conv.title}")
                # åˆ é™¤å¯¹è¯çš„æ‰€æœ‰æ¶ˆæ¯
                db.session.query(Message).filter_by(conversation_id=conv.id).delete()
                # åˆ é™¤å¯¹è¯
                db.session.delete(conv)
            
            db.session.commit()
            logger.info(f"å·²æ¸…ç† {len(conversations_to_delete)} ä¸ªæ—§å¯¹è¯ï¼Œä¿ç•™æœ€è¿‘ {max_conversations} ä¸ªå¯¹è¯")
            
    except Exception as e:
        logger.error(f"æ¸…ç†æ—§å¯¹è¯å¤±è´¥: {str(e)}")
        db.session.rollback()

@api_bp.route('/chat', methods=['POST'])
@login_required
def chat():
    """èŠå¤©API - ä½¿ç”¨LangChainç®¡ç†ä¸Šä¸‹æ–‡ï¼Œç‹¬ç«‹çº¿ç¨‹ä¿å­˜æ¶ˆæ¯"""
    try:
        # ç¡®ä¿æ¶ˆæ¯ä¿å­˜å·¥ä½œçº¿ç¨‹è¿è¡Œ
        start_message_save_worker()
        
        data = request.get_json()
        
        if not data or 'message' not in data:
            return jsonify({
                'success': False,
                'message': 'ç¼ºå°‘æ¶ˆæ¯å†…å®¹'
            }), 400
        
        user_message = data['message'].strip()
        conversation_id = data.get('conversation_id')
        knowledge_base_id = data.get('knowledge_base_id')
        
        if not user_message:
            return jsonify({
                'success': False,
                'message': 'æ¶ˆæ¯ä¸èƒ½ä¸ºç©º'
            }), 400
        
        # è·å–æˆ–åˆ›å»ºå¯¹è¯
        conversation = None
        is_new_conversation = False
        if conversation_id:
            conversation = Conversation.query.filter_by(
                id=conversation_id,
                user_id=current_user.id
            ).first()
        
        if not conversation:
            # åˆ›å»ºæ–°å¯¹è¯å‰ï¼Œå…ˆæ¸…ç†æ—§å¯¹è¯
            cleanup_old_conversations(current_user.id, max_conversations=10)
            
            # åˆ›å»ºæ–°å¯¹è¯
            conversation = Conversation(
                id=str(uuid.uuid4()),
                user_id=current_user.id,
                title=user_message[:50] + ('...' if len(user_message) > 50 else ''),
                model_name='deepseek-chat'
            )
            db.session.add(conversation)
            db.session.flush()  # è·å–ID
            is_new_conversation = True
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦æµå¼å“åº”
        use_stream = request.args.get('stream', 'true').lower() == 'true'
        
        # ä¿å­˜ç”¨æˆ·æ¶ˆæ¯
        user_msg_id = str(uuid.uuid4())
        user_msg = Message(
            id=user_msg_id,
            conversation_id=conversation.id,
            role='user',
            content=user_message,
            token_count=len(user_message.split()),
            created_at=datetime.utcnow()
        )
        db.session.add(user_msg)
        
        # ä½¿ç”¨LangChainä¸Šä¸‹æ–‡æœåŠ¡
        conversation_service = ConversationService()
        
        # è·å–çŸ¥è¯†åº“ä¸Šä¸‹æ–‡
        knowledge_context = None
        if knowledge_base_id:
            kb = KnowledgeBase.query.filter_by(
                id=knowledge_base_id,
                user_id=current_user.id
            ).first()
            if kb:
                knowledge_context = f"æ¥è‡ªçŸ¥è¯†åº“ '{kb.name}' çš„ç›¸å…³å†…å®¹"
        
        if use_stream:
            # ç«‹å³æäº¤ç”¨æˆ·æ¶ˆæ¯
            conversation.message_count += 1
            conversation.updated_at = datetime.utcnow()
            db.session.commit()
            
            @stream_with_context
            def generate_stream():
                ai_response = ""
                ai_msg_id = str(uuid.uuid4())
                debug_info = {
                    'conversation_id': conversation.id,
                    'user_message_id': user_msg_id,
                    'ai_message_id': ai_msg_id,
                    'langchain_enabled': conversation_service.langchain_enabled,
                    'processing_steps': [],
                    'context_info': {},
                    'timing': {},
                    'db_operations': [],
                    'stream_mode': True,
                    'user_id': current_user.id
                }
                
                try:
                    start_time = time.time()
                    
                    # è®°å½•ç”¨æˆ·æ¶ˆæ¯ä¿å­˜
                    debug_info['db_operations'].append({
                        'operation': 'user_message_saved',
                        'timestamp': start_time,
                        'user_msg_id': user_msg_id,
                        'conversation_id': conversation.id,
                        'message_content_length': len(user_message)
                    })
                    
                    # ğŸ”¥ ä¿®å¤BUG2ï¼šç«‹å³å‘é€åˆå§‹åŒ–æ¶ˆæ¯ï¼Œè®©ç”¨æˆ·çŸ¥é“æ­£åœ¨å¤„ç†
                    yield f"data: {json.dumps({'type': 'start', 'conversation_id': conversation.id, 'user_message_id': user_msg_id, 'ai_message_id': ai_msg_id, 'debug_info': debug_info})}\n\n"
                    
                    debug_info['processing_steps'].append(f"[{time.time() - start_time:.3f}s] å¼€å§‹å¤„ç†ç”¨æˆ·æ¶ˆæ¯")
                    
                    # å‘é€å¤„ç†çŠ¶æ€æ›´æ–°
                    yield f"data: {json.dumps({'type': 'processing', 'status': 'æ­£åœ¨å¤„ç†ä¸­...'})}\n\n"
                    
                    if conversation_service.langchain_enabled and conversation_service.langchain_service:
                        # ä½¿ç”¨LangChainè¿›è¡Œå¯¹è¯å¤„ç†
                        debug_info['processing_steps'].append(f"[{time.time() - start_time:.3f}s] ä½¿ç”¨LangChainå¤„ç†ä¸Šä¸‹æ–‡")
                        
                        # è·å–ç³»ç»Ÿæç¤ºè¯
                        system_prompt = conversation_service.get_system_prompt(
                            context_summary=None,  # LangChainä¼šè‡ªåŠ¨å¤„ç†æ‘˜è¦
                            knowledge_context=knowledge_context
                        )
                        
                        debug_info['system_prompt'] = system_prompt
                        debug_info['processing_steps'].append(f"[{time.time() - start_time:.3f}s] ç³»ç»Ÿæç¤ºè¯å‡†å¤‡å®Œæˆ")
                        
                        # è·å–LangChainä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ˆç”¨äºè°ƒè¯•ï¼‰
                        context_info = conversation_service.langchain_service.get_conversation_context(conversation.id)
                        debug_info['context_info'] = context_info
                        debug_info['processing_steps'].append(f"[{time.time() - start_time:.3f}s] LangChainä¸Šä¸‹æ–‡åŠ è½½å®Œæˆ: {context_info.get('message_count', 0)} æ¡æ¶ˆæ¯")
                        
                        # ğŸ”¥ é‡å¤§ä¿®å¤ï¼šä½¿ç”¨çœŸæ­£çš„æµå¼è¾“å‡ºè€Œä¸æ˜¯æ¨¡æ‹Ÿ
                        debug_info['processing_steps'].append(f"[{time.time() - start_time:.3f}s] å¼€å§‹çœŸæ­£çš„æµå¼è¾“å‡º")
                        
                        # ç›´æ¥ä½¿ç”¨DeepSeekçš„æµå¼APIï¼Œä¸é€šè¿‡LangChain
                        service = get_deepseek_service()
                        
                        # è·å–LangChainä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ˆç”¨äºè°ƒè¯•ï¼‰
                        context_info = conversation_service.langchain_service.get_conversation_context(conversation.id)
                        debug_info['context_info'] = context_info
                        debug_info['processing_steps'].append(f"[{time.time() - start_time:.3f}s] LangChainä¸Šä¸‹æ–‡åŠ è½½å®Œæˆ: {context_info.get('message_count', 0)} æ¡æ¶ˆæ¯")
                        
                        # è·å–å¯¹è¯å†å²ç”¨äºæµå¼API
                        conversation_history, _ = conversation_service.get_optimized_context(conversation)
                        conversation_history = [msg for msg in conversation_history if msg.get('content') != user_message]
                        
                        # ä½¿ç”¨çœŸæ­£çš„æµå¼API
                        try:
                            for chunk in service.chat_stream(
                                user_message=user_message,
                                conversation_history=conversation_history,
                                knowledge_context=knowledge_context,
                                system_prompt=system_prompt
                            ):
                                ai_response += chunk
                                yield f"data: {json.dumps({'type': 'content', 'content': chunk})}\n\n"
                            
                            debug_info['langchain_result'] = {
                                'success': True,
                                'response_length': len(ai_response),
                                'context_info': context_info,
                                'stream_mode': True
                            }
                            
                        except Exception as stream_error:
                            # æµå¼è¾“å‡ºå¤±è´¥ï¼Œå›é€€åˆ°éæµå¼
                            logger.error(f"æµå¼è¾“å‡ºå¤±è´¥ï¼Œå›é€€åˆ°éæµå¼: {stream_error}")
                            debug_info['processing_steps'].append(f"[{time.time() - start_time:.3f}s] æµå¼è¾“å‡ºå¤±è´¥ï¼Œå›é€€åˆ°éæµå¼")
                            
                            try:
                                langchain_result = conversation_service.langchain_service.chat_with_langchain(
                                    conversation_id=conversation.id,
                                    user_message=user_message,
                                    system_prompt=system_prompt,
                                    knowledge_context=knowledge_context
                                )
                                
                                if langchain_result['success']:
                                    ai_response = langchain_result['response']
                                    # å¿«é€Ÿå‘é€å“åº”
                                    chunk_size = 30
                                    for i in range(0, len(ai_response), chunk_size):
                                        chunk = ai_response[i:i + chunk_size]
                                        yield f"data: {json.dumps({'type': 'content', 'content': chunk})}\n\n"
                                        time.sleep(0.005)  # æçŸ­å»¶è¿Ÿ
                                        
                                    debug_info['langchain_result'] = {
                                        'success': True,
                                        'response_length': len(ai_response),
                                        'context_info': context_info,
                                        'fallback_mode': True
                                    }
                                else:
                                    debug_info['langchain_result'] = {
                                        'success': False,
                                        'error': langchain_result.get('error', 'æœªçŸ¥é”™è¯¯')
                                    }
                                    ai_response = "æŠ±æ­‰ï¼ŒLangChainå¤„ç†å‡ºç°é—®é¢˜ï¼Œè¯·ç¨åé‡è¯•ã€‚"
                                    yield f"data: {json.dumps({'type': 'content', 'content': ai_response})}\n\n"
                                    
                            except Exception as fallback_error:
                                logger.error(f"LangChainå›é€€ä¹Ÿå¤±è´¥: {fallback_error}")
                                debug_info['langchain_result'] = {
                                    'success': False,
                                    'error': f"æµå¼å’Œå›é€€éƒ½å¤±è´¥: {str(fallback_error)}"
                                }
                                ai_response = "æŠ±æ­‰ï¼Œå¤„ç†å‡ºç°é—®é¢˜ï¼Œè¯·ç¨åé‡è¯•ã€‚"
                                yield f"data: {json.dumps({'type': 'content', 'content': ai_response})}\n\n"
                    
                    else:
                        # å›é€€åˆ°ä¼ ç»Ÿæ–¹å¼ï¼ˆå¦‚æœLangChainæœªå¯ç”¨ï¼‰
                        debug_info['processing_steps'].append(f"[{time.time() - start_time:.3f}s] ä½¿ç”¨ä¼ ç»Ÿæ–¹å¼å¤„ç†ä¸Šä¸‹æ–‡")
                        
                        # è·å–ä¼˜åŒ–åçš„ä¸Šä¸‹æ–‡
                        conversation_history, context_summary = conversation_service.get_optimized_context(conversation)
                        conversation_history = [msg for msg in conversation_history if msg.get('content') != user_message]
                        
                        debug_info['traditional_context'] = {
                            'history_count': len(conversation_history),
                            'has_summary': context_summary is not None
                        }
                        
                        system_prompt = conversation_service.get_system_prompt(
                            context_summary=context_summary,
                            knowledge_context=knowledge_context
                        )
                        
                        # è·å–DeepSeekæœåŠ¡å¹¶æµå¼è·å–å“åº”
                        service = get_deepseek_service()
                        for chunk in service.chat_stream(
                            user_message=user_message,
                            conversation_history=conversation_history,
                            knowledge_context=knowledge_context,
                            system_prompt=system_prompt
                        ):
                            ai_response += chunk
                            yield f"data: {json.dumps({'type': 'content', 'content': chunk})}\n\n"
                    
                    # å®Œæˆå¤„ç†
                    debug_info['timing']['total_time'] = time.time() - start_time
                    debug_info['processing_steps'].append(f"[{debug_info['timing']['total_time']:.3f}s] å¯¹è¯å¤„ç†å®Œæˆï¼Œå‡†å¤‡ä¿å­˜AIæ¶ˆæ¯")
                    
                    # ğŸ”¥ å…³é”®æ”¹è¿›ï¼šå°†AIæ¶ˆæ¯ä¿å­˜ä»»åŠ¡åŠ å…¥ç‹¬ç«‹çº¿ç¨‹é˜Ÿåˆ—
                    if ai_response.strip():  # ç¡®ä¿æœ‰æœ‰æ•ˆå“åº”
                        save_data = {
                            'conversation_id': conversation.id,
                            'ai_msg_id': ai_msg_id,
                            'ai_response': ai_response,
                            'debug_info': debug_info,
                            'user_id': current_user.id,
                            'retry_count': 0
                        }
                        
                        # åŠ å…¥ä¿å­˜é˜Ÿåˆ—ï¼ˆå¼‚æ­¥å¤„ç†ï¼‰
                        message_save_queue.put(save_data)
                        
                        debug_info['processing_steps'].append(f"[{debug_info['timing']['total_time']:.3f}s] AIæ¶ˆæ¯å·²åŠ å…¥ä¿å­˜é˜Ÿåˆ—")
                        debug_info['db_operations'].append({
                            'operation': 'ai_message_queued_for_save',
                            'timestamp': time.time(),
                            'ai_msg_id': ai_msg_id,
                            'response_length': len(ai_response),
                            'queue_size': message_save_queue.qsize()
                        })
                    else:
                        debug_info['processing_steps'].append(f"[{debug_info['timing']['total_time']:.3f}s] âš ï¸ AIå“åº”ä¸ºç©ºï¼Œè·³è¿‡ä¿å­˜")
                    
                    debug_info['final_stats'] = {
                        'ai_response_length': len(ai_response),
                        'processing_complete': True,
                        'save_queued': bool(ai_response.strip()),
                        'queue_size': message_save_queue.qsize()
                    }
                    
                    # å‘é€å®Œæˆæ¶ˆæ¯ï¼ŒåŒ…å«è°ƒè¯•ä¿¡æ¯
                    yield f"data: {json.dumps({'type': 'done', 'message_id': ai_msg_id, 'full_response': ai_response, 'debug_info': debug_info})}\n\n"
                    
                except Exception as e:
                    logger.error(f"æµå¼å“åº”å¼‚å¸¸: {str(e)}")
                    debug_info['error'] = {
                        'message': str(e),
                        'type': type(e).__name__,
                        'traceback': traceback.format_exc(),
                        'occurred_at': time.time() - start_time if 'start_time' in locals() else 'unknown'
                    }
                    debug_info['db_operations'].append({
                        'operation': 'stream_error',
                        'timestamp': time.time(),
                        'error': str(e),
                        'error_type': type(e).__name__
                    })
                    yield f"data: {json.dumps({'type': 'error', 'error': str(e), 'debug_info': debug_info})}\n\n"
            
            return Response(
                generate_stream(),
                mimetype='text/event-stream',
                headers={
                    'Cache-Control': 'no-cache',
                    'Connection': 'keep-alive',
                    'Access-Control-Allow-Origin': '*',
                    'Access-Control-Allow-Headers': 'Cache-Control'
                }
            )
        
        else:
            # éæµå¼å“åº”ï¼ˆä½¿ç”¨LangChainï¼‰
            if conversation_service.langchain_enabled and conversation_service.langchain_service:
                system_prompt = conversation_service.get_system_prompt(
                    context_summary=None,
                    knowledge_context=knowledge_context
                )
                
                langchain_result = conversation_service.langchain_service.chat_with_langchain(
                    conversation_id=conversation.id,
                    user_message=user_message,
                    system_prompt=system_prompt,
                    knowledge_context=knowledge_context
                )
                
                if langchain_result['success']:
                    ai_response = langchain_result['response']
                else:
                    return jsonify({
                        'success': False,
                        'message': f"LangChainå¤„ç†å¤±è´¥: {langchain_result.get('error', 'æœªçŸ¥é”™è¯¯')}"
                    }), 500
            else:
                # å›é€€åˆ°ä¼ ç»Ÿæ–¹å¼
                conversation_history, context_summary = conversation_service.get_optimized_context(conversation)
                conversation_history = [msg for msg in conversation_history if msg.get('content') != user_message]
                
                system_prompt = conversation_service.get_system_prompt(
                    context_summary=context_summary,
                    knowledge_context=knowledge_context
                )
                
                service = get_deepseek_service()
                api_result = service.chat_with_context(
                    user_message=user_message,
                    conversation_history=conversation_history,
                    knowledge_context=knowledge_context,
                    system_prompt=system_prompt,
                    stream=False
                )
                
                if not api_result['success']:
                    return jsonify({
                        'success': False,
                        'message': f"AIæœåŠ¡è°ƒç”¨å¤±è´¥: {api_result.get('error', 'æœªçŸ¥é”™è¯¯')}"
                    }), 500
                
                ai_response = api_result['data']['choices'][0]['message']['content']
            
            # ä¿å­˜AIæ¶ˆæ¯ï¼ˆéæµå¼æ¨¡å¼ç›´æ¥ä¿å­˜ï¼‰
            ai_msg_id = str(uuid.uuid4())
            ai_msg = Message(
                id=ai_msg_id,
                conversation_id=conversation.id,
                role='assistant',
                content=ai_response,
                token_count=len(ai_response.split()),
                created_at=datetime.utcnow()
            )
            db.session.add(ai_msg)
            conversation.message_count += 1
            conversation.updated_at = datetime.utcnow()
            db.session.commit()
            
            return jsonify({
                'success': True,
                'response': ai_response,
                'conversation_id': conversation.id,
                'message_id': ai_msg_id,
                'langchain_enabled': conversation_service.langchain_enabled
            })
            
    except Exception as e:
        logger.error(f"èŠå¤©APIå¼‚å¸¸: {str(e)}")
        db.session.rollback()
        return jsonify({
            'success': False,
            'message': f'æœåŠ¡å™¨é”™è¯¯: {str(e)}'
        }), 500

@api_bp.route('/chat_simple', methods=['POST'])
@login_required
def chat_simple():
    """ç®€å•èŠå¤©APIï¼ˆéæµå¼ï¼‰"""
    try:
        data = request.get_json()
        
        if not data or 'message' not in data:
            return jsonify({
                'success': False,
                'message': 'ç¼ºå°‘æ¶ˆæ¯å†…å®¹'
            }), 400
        
        user_message = data['message'].strip()
        
        if not user_message:
            return jsonify({
                'success': False,
                'message': 'æ¶ˆæ¯ä¸èƒ½ä¸ºç©º'
            }), 400
        
        # ç›´æ¥è°ƒç”¨DeepSeek APIè¿›è¡Œæµ‹è¯•
        service = get_deepseek_service()
        result = service.simple_chat(user_message)
        
        if result['success']:
            return jsonify({
                'success': True,
                'response': result['response'],
                'usage': result.get('usage', {})
            })
        else:
            return jsonify({
                'success': False,
                'message': f"AIæœåŠ¡è°ƒç”¨å¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}"
            }), 500
            
    except Exception as e:
        logger.error(f"ç®€å•èŠå¤©APIå¼‚å¸¸: {str(e)}")
        return jsonify({
            'success': False,
            'message': f'æœåŠ¡å™¨å†…éƒ¨é”™è¯¯: {str(e)}'
        }), 500

@api_bp.route('/knowledge_bases', methods=['GET'])
@login_required
def get_knowledge_bases():
    """è·å–ç”¨æˆ·çŸ¥è¯†åº“åˆ—è¡¨"""
    try:
        knowledge_bases = current_user.knowledge_bases.filter_by(is_active=True).all()
        return jsonify({
            'success': True,
            'knowledge_bases': [kb.to_dict() for kb in knowledge_bases]
        })
    except Exception as e:
        logger.error(f"è·å–çŸ¥è¯†åº“åˆ—è¡¨å¤±è´¥: {str(e)}")
        return jsonify({
            'success': False,
            'message': 'è·å–çŸ¥è¯†åº“åˆ—è¡¨å¤±è´¥'
        }), 500

@api_bp.route('/conversations', methods=['GET'])
@login_required
def get_conversations():
    """è·å–å¯¹è¯åˆ—è¡¨"""
    try:
        # åªè·å–æ´»è·ƒçš„å¯¹è¯ï¼ŒæŒ‰æ›´æ–°æ—¶é—´å€’åºæ’åˆ—ï¼ˆæœ€æ–°çš„åœ¨å‰ï¼‰
        conversations = db.session.query(Conversation)\
            .filter_by(user_id=current_user.id, is_active=True)\
            .order_by(Conversation.updated_at.desc())\
            .all()
        
        conversation_list = []
        for conv in conversations:
            # è·å–æ¶ˆæ¯ç»Ÿè®¡
            message_count = db.session.query(Message)\
                .filter_by(conversation_id=conv.id)\
                .count()
            
            # è·å–æœ€åä¸€æ¡æ¶ˆæ¯çš„é¢„è§ˆ
            last_message = db.session.query(Message)\
                .filter_by(conversation_id=conv.id)\
                .order_by(Message.created_at.desc())\
                .first()
            
            last_message_preview = None
            last_message_time = None
            if last_message:
                # æˆªå–æ¶ˆæ¯å†…å®¹çš„å‰50ä¸ªå­—ç¬¦ä½œä¸ºé¢„è§ˆ
                preview_content = last_message.content[:50]
                if len(last_message.content) > 50:
                    preview_content += '...'
                last_message_preview = f"{'ğŸ‘¤' if last_message.role == 'user' else 'ğŸ¤–'} {preview_content}"
                last_message_time = last_message.created_at.isoformat()
            
            conversation_list.append({
                'id': conv.id,
                'title': conv.title or 'æ–°å¯¹è¯',
                'created_at': conv.created_at.isoformat(),
                'updated_at': conv.updated_at.isoformat(),
                'message_count': message_count,
                'last_message_preview': last_message_preview,
                'last_message_time': last_message_time,
                'model_name': conv.model_name
            })
        
        return jsonify({
            'success': True,
            'conversations': conversation_list,
            'total_count': len(conversation_list)
        })
    
    except Exception as e:
        logger.error(f"è·å–å¯¹è¯åˆ—è¡¨å¤±è´¥: {str(e)}")
        return jsonify({
            'success': False,
            'message': f'è·å–å¯¹è¯åˆ—è¡¨å¤±è´¥: {str(e)}'
        }), 500

@api_bp.route('/conversations/<conversation_id>/messages', methods=['GET'])
@login_required
def get_conversation_messages(conversation_id):
    """è·å–å¯¹è¯çš„æ‰€æœ‰æ¶ˆæ¯"""
    try:
        conversation = db.session.get(Conversation, conversation_id)
        if not conversation or conversation.user_id != current_user.id:
            return jsonify({
                'success': False,
                'message': 'å¯¹è¯ä¸å­˜åœ¨'
            }), 404
        
        # å¼ºåˆ¶åˆ·æ–°æ•°æ®åº“è¿æ¥ï¼Œé¿å…ç¼“å­˜é—®é¢˜
        db.session.expunge_all()
        
        # æŒ‰åˆ›å»ºæ—¶é—´å‡åºæ’åˆ—ï¼Œç¡®ä¿æ¶ˆæ¯æŒ‰å¯¹è¯é¡ºåºæ˜¾ç¤º
        messages = db.session.query(Message)\
            .filter_by(conversation_id=conversation_id)\
            .order_by(Message.created_at.asc())\
            .all()
        
        logger.info(f"æŸ¥è¯¢å¯¹è¯ {conversation_id} çš„æ¶ˆæ¯ï¼Œå…±æ‰¾åˆ° {len(messages)} æ¡æ¶ˆæ¯")
        for i, msg in enumerate(messages):
            logger.info(f"æ¶ˆæ¯ {i+1}: ID={msg.id}, role={msg.role}, created_at={msg.created_at}, content={msg.content[:50]}...")
        
        message_list = []
        for i, msg in enumerate(messages):
            # è®¡ç®—æ¶ˆæ¯çš„ç›¸å¯¹æ—¶é—´å·®
            time_diff = None
            if i > 0:
                prev_msg = messages[i-1]
                diff_seconds = (msg.created_at - prev_msg.created_at).total_seconds()
                if diff_seconds > 60:  # è¶…è¿‡1åˆ†é’Ÿæ˜¾ç¤ºæ—¶é—´å·®
                    if diff_seconds < 3600:  # å°äº1å°æ—¶
                        time_diff = f"{int(diff_seconds // 60)}åˆ†é’Ÿå"
                    elif diff_seconds < 86400:  # å°äº1å¤©
                        time_diff = f"{int(diff_seconds // 3600)}å°æ—¶å"
                    else:  # è¶…è¿‡1å¤©
                        time_diff = f"{int(diff_seconds // 86400)}å¤©å"
            
            message_list.append({
                'id': msg.id,
                'role': msg.role,
                'content': msg.content,
                'token_count': msg.token_count or 0,
                'created_at': msg.created_at.isoformat(),
                'created_at_formatted': msg.created_at.strftime('%Y-%m-%d %H:%M:%S'),
                'time_diff': time_diff,
                'sequence_number': i + 1  # æ¶ˆæ¯åºå·
            })
        
        return jsonify({
            'success': True,
            'messages': message_list,
            'conversation': {
                'id': conversation.id,
                'title': conversation.title,
                'created_at': conversation.created_at.isoformat(),
                'updated_at': conversation.updated_at.isoformat(),
                'message_count': len(message_list),
                'model_name': conversation.model_name
            }
        })
    
    except Exception as e:
        logger.error(f"è·å–å¯¹è¯æ¶ˆæ¯å¤±è´¥: {str(e)}")
        return jsonify({
            'success': False,
            'message': f'è·å–å¯¹è¯æ¶ˆæ¯å¤±è´¥: {str(e)}'
        }), 500

@api_bp.route('/conversations/<conversation_id>', methods=['DELETE'])
@login_required
def delete_conversation(conversation_id):
    """åˆ é™¤å¯¹è¯åŠå…¶æ‰€æœ‰æ¶ˆæ¯"""
    try:
        conversation = db.session.get(Conversation, conversation_id)
        if not conversation or conversation.user_id != current_user.id:
            return jsonify({
                'success': False,
                'message': 'å¯¹è¯ä¸å­˜åœ¨'
            }), 404
        
        # åˆ é™¤å¯¹è¯çš„æ‰€æœ‰æ¶ˆæ¯
        db.session.query(Message)\
            .filter_by(conversation_id=conversation_id)\
            .delete()
        
        # åˆ é™¤å¯¹è¯
        db.session.delete(conversation)
        db.session.commit()
        
        return jsonify({
            'success': True,
            'message': 'å¯¹è¯å·²åˆ é™¤'
        })
    
    except Exception as e:
        db.session.rollback()
        logger.error(f"åˆ é™¤å¯¹è¯å¤±è´¥: {str(e)}")
        return jsonify({
            'success': False,
            'message': f'åˆ é™¤å¯¹è¯å¤±è´¥: {str(e)}'
        }), 500

@api_bp.route('/conversations/<conversation_id>/title', methods=['PUT'])
@login_required
def update_conversation_title(conversation_id):
    """æ›´æ–°å¯¹è¯æ ‡é¢˜"""
    try:
        data = request.get_json()
        new_title = data.get('title', '').strip()
        
        if not new_title:
            return jsonify({
                'success': False,
                'message': 'æ ‡é¢˜ä¸èƒ½ä¸ºç©º'
            }), 400
        
        conversation = db.session.get(Conversation, conversation_id)
        if not conversation or conversation.user_id != current_user.id:
            return jsonify({
                'success': False,
                'message': 'å¯¹è¯ä¸å­˜åœ¨'
            }), 404
        
        conversation.title = new_title
        conversation.updated_at = datetime.utcnow()
        db.session.commit()
        
        return jsonify({
            'success': True,
            'message': 'æ ‡é¢˜å·²æ›´æ–°'
        })
    
    except Exception as e:
        db.session.rollback()
        logger.error(f"æ›´æ–°å¯¹è¯æ ‡é¢˜å¤±è´¥: {str(e)}")
        return jsonify({
            'success': False,
            'message': f'æ›´æ–°å¯¹è¯æ ‡é¢˜å¤±è´¥: {str(e)}'
        }), 500

@api_bp.route('/test_deepseek', methods=['GET'])
@login_required
def test_deepseek():
    """æµ‹è¯•DeepSeek APIè¿æ¥"""
    try:
        service = get_deepseek_service()
        result = service.test_connection()
        return jsonify(result)
    except Exception as e:
        logger.error(f"æµ‹è¯•DeepSeek APIå¤±è´¥: {str(e)}")
        return jsonify({
            'success': False,
            'message': f'æµ‹è¯•å¤±è´¥: {str(e)}'
        }), 500

@api_bp.route('/stats', methods=['GET'])
@login_required
def get_stats():
    """è·å–ç”¨æˆ·ç»Ÿè®¡ä¿¡æ¯"""
    try:
        knowledge_bases = current_user.knowledge_bases.filter_by(is_active=True).all()
        
        stats = {
            'knowledge_bases_count': len(knowledge_bases),
            'conversations_count': current_user.conversations.filter_by(is_active=True).count(),
            'total_documents': sum(kb.document_count for kb in knowledge_bases),
            'total_messages': sum(conv.message_count for conv in current_user.conversations),
            'total_storage_size': sum(kb.total_size for kb in knowledge_bases)
        }
        
        return jsonify({
            'success': True,
            'stats': stats
        })
    except Exception as e:
        logger.error(f"è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {str(e)}")
        return jsonify({
            'success': False,
            'message': 'è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥'
        }), 500

@api_bp.route('/save_message', methods=['POST'])
@login_required
def save_message():
    """ä¿å­˜AIå“åº”æ¶ˆæ¯"""
    try:
        data = request.get_json()
        
        if not data:
            return jsonify({'success': False, 'message': 'ç¼ºå°‘æ•°æ®'}), 400
        
        conversation_id = data.get('conversation_id')
        message_id = data.get('message_id')
        content = data.get('content')
        role = data.get('role', 'assistant')
        
        if not all([conversation_id, message_id, content]):
            return jsonify({'success': False, 'message': 'ç¼ºå°‘å¿…éœ€å­—æ®µ'}), 400
        
        # ç¡®ä¿åœ¨åº”ç”¨ä¸Šä¸‹æ–‡ä¸­è¿›è¡Œæ•°æ®åº“æ“ä½œ
        with current_app.app_context():
            # éªŒè¯å¯¹è¯æ‰€æœ‰æƒ
            conversation = Conversation.query.filter_by(
                id=conversation_id,
                user_id=current_user.id
            ).first()
            
            if not conversation:
                return jsonify({'success': False, 'message': 'å¯¹è¯ä¸å­˜åœ¨'}), 404
            
            # æ£€æŸ¥æ¶ˆæ¯æ˜¯å¦å·²å­˜åœ¨ï¼Œé¿å…é‡å¤ä¿å­˜
            existing_message = Message.query.filter_by(id=message_id).first()
            if existing_message:
                logger.info(f"æ¶ˆæ¯ {message_id} å·²å­˜åœ¨ï¼Œè·³è¿‡ä¿å­˜")
                return jsonify({'success': True, 'message': 'æ¶ˆæ¯å·²å­˜åœ¨'})
            
            # ä¿å­˜æ¶ˆæ¯
            message = Message(
                id=message_id,
                conversation_id=conversation_id,
                role=role,
                content=content,
                token_count=0,
                created_at=datetime.utcnow()  # æ˜ç¡®è®¾ç½®åˆ›å»ºæ—¶é—´
            )
            db.session.add(message)
            
            # æ›´æ–°å¯¹è¯ç»Ÿè®¡
            conversation.message_count += 1
            conversation.updated_at = datetime.utcnow()
            
            db.session.commit()
            logger.info(f"å‰ç«¯å¤‡ä»½ä¿å­˜æ¶ˆæ¯æˆåŠŸ: {message_id}")
        
        return jsonify({'success': True})
        
    except Exception as e:
        logger.error(f"ä¿å­˜æ¶ˆæ¯å¤±è´¥: {str(e)}")
        try:
            with current_app.app_context():
                db.session.rollback()
        except:
            pass
        return jsonify({'success': False, 'message': str(e)}), 500

@api_bp.route('/test_context_summary', methods=['POST'])
@login_required
def test_context_summary():
    """æµ‹è¯•ä¸Šä¸‹æ–‡æ€»ç»“åŠŸèƒ½"""
    try:
        data = request.get_json()
        
        if not data or 'conversation_id' not in data:
            return jsonify({
                'success': False,
                'message': 'ç¼ºå°‘å¯¹è¯ID'
            }), 400
        
        conversation_id = data['conversation_id']
        
        # éªŒè¯å¯¹è¯æ‰€æœ‰æƒ
        conversation = Conversation.query.filter_by(
            id=conversation_id,
            user_id=current_user.id
        ).first()
        
        if not conversation:
            return jsonify({
                'success': False,
                'message': 'å¯¹è¯ä¸å­˜åœ¨'
            }), 404
        
        # åˆå§‹åŒ–å¯¹è¯æœåŠ¡
        conversation_service = ConversationService()
        
        # è·å–ä¼˜åŒ–åçš„ä¸Šä¸‹æ–‡
        optimized_messages, context_summary = conversation_service.get_optimized_context(conversation)
        
        # ç»Ÿè®¡ä¿¡æ¯
        total_messages = conversation.messages.count()
        user_message_count = conversation_service.count_user_messages(conversation)
        should_summarize = conversation_service.should_summarize_context(conversation)
        
        return jsonify({
            'success': True,
            'conversation_id': conversation_id,
            'total_messages': total_messages,
            'user_message_count': user_message_count,
            'optimized_message_count': len(optimized_messages),
            'should_summarize': should_summarize,
            'has_summary': context_summary is not None,
            'context_summary': context_summary,
            'optimized_messages': optimized_messages[-5:] if optimized_messages else []  # åªè¿”å›æœ€å5æ¡ç”¨äºé¢„è§ˆ
        })
        
    except Exception as e:
        logger.error(f"æµ‹è¯•ä¸Šä¸‹æ–‡æ€»ç»“å¤±è´¥: {str(e)}")
        return jsonify({
            'success': False,
            'message': f'æµ‹è¯•å¤±è´¥: {str(e)}'
        }), 500

@api_bp.route('/conversation_stats', methods=['GET'])
@login_required
def get_conversation_stats():
    """è·å–å¯¹è¯ç»Ÿè®¡ä¿¡æ¯"""
    try:
        conversation_id = request.args.get('conversation_id')
        
        if not conversation_id:
            return jsonify({
                'success': False,
                'message': 'ç¼ºå°‘å¯¹è¯ID'
            }), 400
        
        conversation = Conversation.query.filter_by(
            id=conversation_id,
            user_id=current_user.id
        ).first()
        
        if not conversation:
            return jsonify({
                'success': False,
                'message': 'å¯¹è¯ä¸å­˜åœ¨'
            }), 404
        
        conversation_service = ConversationService()
        
        # åŸºç¡€ç»Ÿè®¡
        total_messages = conversation.messages.count()
        user_messages = conversation.messages.filter_by(role='user').count()
        ai_messages = conversation.messages.filter_by(role='assistant').count()
        
        # ä¸Šä¸‹æ–‡ç®¡ç†ç»Ÿè®¡
        should_summarize = conversation_service.should_summarize_context(conversation)
        optimized_messages, has_summary = conversation_service.get_optimized_context(conversation)
        
        # LangChain ä¸Šä¸‹æ–‡åˆ†æ
        context_analysis = conversation_service.get_context_analysis(conversation_id)
        
        return jsonify({
            'success': True,
            'conversation_id': conversation_id,
            'stats': {
                'total_messages': total_messages,
                'user_messages': user_messages,
                'ai_messages': ai_messages,
                'should_summarize': should_summarize,
                'optimized_message_count': len(optimized_messages) if optimized_messages else 0,
                'has_context_summary': has_summary is not None,
                'summary_rounds_config': conversation_service.summary_rounds,
                'max_context_messages': conversation_service.max_context_messages,
                'langchain_enabled': conversation_service.langchain_enabled
            },
            'context_analysis': context_analysis
        })
        
    except Exception as e:
        logger.error(f"è·å–å¯¹è¯ç»Ÿè®¡ä¿¡æ¯å¼‚å¸¸: {str(e)}")
        return jsonify({
            'success': False,
            'message': f'æœåŠ¡å™¨é”™è¯¯: {str(e)}'
        }), 500

@api_bp.route('/langchain/context/<conversation_id>', methods=['GET'])
@login_required
def get_langchain_context(conversation_id):
    """è·å– LangChain ç®¡ç†çš„å¯¹è¯ä¸Šä¸‹æ–‡"""
    try:
        # éªŒè¯å¯¹è¯æ‰€æœ‰æƒ
        conversation = Conversation.query.filter_by(
            id=conversation_id,
            user_id=current_user.id
        ).first()
        
        if not conversation:
            return jsonify({
                'success': False,
                'message': 'å¯¹è¯ä¸å­˜åœ¨æˆ–æ— æƒé™è®¿é—®'
            }), 404
        
        conversation_service = ConversationService()
        
        if not conversation_service.langchain_enabled:
            return jsonify({
                'success': False,
                'message': 'LangChain åŠŸèƒ½æœªå¯ç”¨'
            }), 400
        
        # è·å– LangChain ä¸Šä¸‹æ–‡ä¿¡æ¯
        context_info = conversation_service.langchain_service.get_conversation_context(conversation_id)
        
        return jsonify({
            'success': True,
            'context_info': context_info
        })
        
    except Exception as e:
        logger.error(f"è·å– LangChain ä¸Šä¸‹æ–‡å¼‚å¸¸: {str(e)}")
        return jsonify({
            'success': False,
            'message': f'æœåŠ¡å™¨é”™è¯¯: {str(e)}'
        }), 500

@api_bp.route('/langchain/summary/<conversation_id>', methods=['GET'])
@login_required
def get_langchain_summary(conversation_id):
    """è·å– LangChain ç”Ÿæˆçš„å¯¹è¯æ‘˜è¦"""
    try:
        # éªŒè¯å¯¹è¯æ‰€æœ‰æƒ
        conversation = Conversation.query.filter_by(
            id=conversation_id,
            user_id=current_user.id
        ).first()
        
        if not conversation:
            return jsonify({
                'success': False,
                'message': 'å¯¹è¯ä¸å­˜åœ¨æˆ–æ— æƒé™è®¿é—®'
            }), 404
        
        conversation_service = ConversationService()
        
        if not conversation_service.langchain_enabled:
            return jsonify({
                'success': False,
                'message': 'LangChain åŠŸèƒ½æœªå¯ç”¨'
            }), 400
        
        # è·å–å¯¹è¯æ‘˜è¦
        summary = conversation_service.langchain_service.get_conversation_summary(conversation_id)
        
        return jsonify({
            'success': True,
            'conversation_id': conversation_id,
            'summary': summary,
            'has_summary': summary is not None
        })
        
    except Exception as e:
        logger.error(f"è·å– LangChain æ‘˜è¦å¼‚å¸¸: {str(e)}")
        return jsonify({
            'success': False,
            'message': f'æœåŠ¡å™¨é”™è¯¯: {str(e)}'
        }), 500

@api_bp.route('/langchain/analyze/<conversation_id>', methods=['GET'])
@login_required
def analyze_langchain_context(conversation_id):
    """åˆ†æ LangChain ç®¡ç†çš„å¯¹è¯ä¸Šä¸‹æ–‡"""
    try:
        # éªŒè¯å¯¹è¯æ‰€æœ‰æƒ
        conversation = Conversation.query.filter_by(
            id=conversation_id,
            user_id=current_user.id
        ).first()
        
        if not conversation:
            return jsonify({
                'success': False,
                'message': 'å¯¹è¯ä¸å­˜åœ¨æˆ–æ— æƒé™è®¿é—®'
            }), 404
        
        conversation_service = ConversationService()
        
        # è·å–ä¸Šä¸‹æ–‡åˆ†æ
        analysis = conversation_service.get_context_analysis(conversation_id)
        
        return jsonify({
            'success': True,
            'analysis': analysis
        })
        
    except Exception as e:
        logger.error(f"åˆ†æ LangChain ä¸Šä¸‹æ–‡å¼‚å¸¸: {str(e)}")
        return jsonify({
            'success': False,
            'message': f'æœåŠ¡å™¨é”™è¯¯: {str(e)}'
        }), 500

@api_bp.route('/langchain/config', methods=['GET'])
@login_required
def get_langchain_config():
    """è·å– LangChain é…ç½®ä¿¡æ¯"""
    try:
        from config.settings import Config
        
        config_info = {
            'enabled': BaseConfig.LANGCHAIN_ENABLED,
            'memory_type': BaseConfig.LANGCHAIN_MEMORY_TYPE,
            'max_token_limit': BaseConfig.LANGCHAIN_MAX_TOKEN_LIMIT,
            'window_size': BaseConfig.LANGCHAIN_WINDOW_SIZE,
            'debug': BaseConfig.LANGCHAIN_DEBUG,
            'verbose': BaseConfig.LANGCHAIN_VERBOSE,
            'conversation_summary_rounds': BaseConfig.CONVERSATION_SUMMARY_ROUNDS,
            'max_context_messages': BaseConfig.MAX_CONTEXT_MESSAGES,
            'conversation_summary_token_limit': BaseConfig.CONVERSATION_SUMMARY_TOKEN_LIMIT
        }
        
        return jsonify({
            'success': True,
            'config': config_info
        })
        
    except Exception as e:
        logger.error(f"è·å– LangChain é…ç½®å¼‚å¸¸: {str(e)}")
        return jsonify({
            'success': False,
            'message': f'æœåŠ¡å™¨é”™è¯¯: {str(e)}'
        }), 500

@api_bp.route('/langchain/test', methods=['POST'])
@login_required
def test_langchain_integration():
    """æµ‹è¯• LangChain é›†æˆåŠŸèƒ½"""
    try:
        data = request.get_json()
        test_message = data.get('message', 'ä½ å¥½ï¼Œè¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æ¶ˆæ¯')
        conversation_id = data.get('conversation_id')
        
        if not conversation_id:
            return jsonify({
                'success': False,
                'message': 'ç¼ºå°‘å¯¹è¯ID'
            }), 400
        
        # éªŒè¯å¯¹è¯æ‰€æœ‰æƒ
        conversation = Conversation.query.filter_by(
            id=conversation_id,
            user_id=current_user.id
        ).first()
        
        if not conversation:
            return jsonify({
                'success': False,
                'message': 'å¯¹è¯ä¸å­˜åœ¨æˆ–æ— æƒé™è®¿é—®'
            }), 404
        
        conversation_service = ConversationService()
        
        if not conversation_service.langchain_enabled:
            return jsonify({
                'success': False,
                'message': 'LangChain åŠŸèƒ½æœªå¯ç”¨'
            }), 400
        
        # ä½¿ç”¨ LangChain è¿›è¡Œæµ‹è¯•å¯¹è¯
        result = conversation_service.langchain_service.chat_with_langchain(
            conversation_id=conversation_id,
            user_message=test_message,
            system_prompt="ä½ æ˜¯SuperRAGæ™ºèƒ½åŠ©æ‰‹çš„æµ‹è¯•æ¨¡å¼ã€‚è¯·ç®€çŸ­å›åº”ç”¨æˆ·çš„æµ‹è¯•æ¶ˆæ¯ã€‚"
        )
        
        return jsonify({
            'success': True,
            'test_result': result
        })
        
    except Exception as e:
        logger.error(f"æµ‹è¯• LangChain é›†æˆå¼‚å¸¸: {str(e)}")
        return jsonify({
            'success': False,
            'message': f'æœåŠ¡å™¨é”™è¯¯: {str(e)}'
        }), 500

@api_bp.route('/message_save_status', methods=['GET'])
@login_required
def get_message_save_status():
    """è·å–æ¶ˆæ¯ä¿å­˜é˜Ÿåˆ—çŠ¶æ€"""
    try:
        global message_save_thread, message_save_queue
        
        thread_status = "æœªå¯åŠ¨"
        if message_save_thread:
            if message_save_thread.is_alive():
                thread_status = "è¿è¡Œä¸­"
            else:
                thread_status = "å·²åœæ­¢"
        
        status = {
            "queue_size": message_save_queue.qsize(),
            "thread_status": thread_status,
            "thread_id": message_save_thread.ident if message_save_thread else None,
            "thread_name": message_save_thread.name if message_save_thread else None,
            "queue_empty": message_save_queue.empty(),
            "current_time": datetime.now().isoformat()
        }
        
        return jsonify({
            "success": True,
            "status": status
        })
        
    except Exception as e:
        return jsonify({
            "success": False,
            "error": str(e)
        }), 500

@api_bp.route('/test_multi_turn', methods=['POST'])
@login_required 
def test_multi_turn():
    """æµ‹è¯•å¤šè½®å¯¹è¯åŠŸèƒ½"""
    try:
        data = request.get_json()
        if not data or 'messages' not in data:
            return jsonify({
                'success': False,
                'error': 'ç¼ºå°‘æ¶ˆæ¯æ•°æ®'
            }), 400
        
        messages = data['messages']
        conversation_id = data.get('conversation_id')
        
        # è·å–å¯¹è¯æœåŠ¡
        conversation_service = ConversationService()
        
        responses = []
        for i, msg in enumerate(messages):
            user_message = msg.get('content', '')
            if not user_message:
                continue
                
            # è·å–æˆ–åˆ›å»ºå¯¹è¯
            conversation = None
            if conversation_id:
                conversation = Conversation.query.filter_by(
                    id=conversation_id,
                    user_id=current_user.id
                ).first()
            
            if not conversation:
                conversation = Conversation(
                    id=str(uuid.uuid4()),
                    user_id=current_user.id,
                    title=f"æµ‹è¯•å¯¹è¯ {datetime.now().strftime('%H:%M:%S')}",
                    model_name='deepseek-chat',
                    created_at=datetime.utcnow(),
                    updated_at=datetime.utcnow()
                )
                db.session.add(conversation)
                db.session.commit()
                conversation_id = conversation.id
            
            # ä¿å­˜ç”¨æˆ·æ¶ˆæ¯
            user_msg = Message(
                id=str(uuid.uuid4()),
                conversation_id=conversation_id,
                role='user',
                content=user_message,
                token_count=len(user_message.split()),
                created_at=datetime.utcnow()
            )
            db.session.add(user_msg)
            
            # è·å–AIå“åº”
            context_summary = conversation_service.get_conversation_summary(conversation.id) if i > 0 else None
            system_prompt = conversation_service.get_system_prompt(
                context_summary=context_summary
            )
            
            service = get_deepseek_service()
            api_result = service.chat_with_context(
                user_message=user_message,
                conversation_history=[],
                system_prompt=system_prompt
            )
            
            if api_result['success']:
                ai_response = api_result['data']['choices'][0]['message']['content']
                
                # ä¿å­˜AIæ¶ˆæ¯
                ai_msg = Message(
                    id=str(uuid.uuid4()),
                    conversation_id=conversation_id,
                    role='assistant',
                    content=ai_response,
                    token_count=len(ai_response.split()),
                    created_at=datetime.utcnow()
                )
                db.session.add(ai_msg)
                
                # æ›´æ–°å¯¹è¯
                conversation.message_count += 2
                conversation.updated_at = datetime.utcnow()
                
                db.session.commit()
                
                responses.append({
                    'round': i + 1,
                    'user_message': user_message,
                    'ai_response': ai_response,
                    'success': True
                })
            else:
                responses.append({
                    'round': i + 1,
                    'user_message': user_message,
                    'error': api_result['error'],
                    'success': False
                })
        
        return jsonify({
            'success': True,
            'conversation_id': conversation_id,
            'responses': responses,
            'total_rounds': len(responses)
        })
        
    except Exception as e:
        logger.error(f"å¤šè½®å¯¹è¯æµ‹è¯•å¤±è´¥: {str(e)}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

@api_bp.route('/conversation_database_info/<conversation_id>', methods=['GET'])
@login_required
def get_conversation_database_info(conversation_id):
    """è·å–å¯¹è¯çš„å®Œæ•´æ•°æ®åº“ä¿¡æ¯ï¼Œç”¨äºè°ƒè¯•é¡µé¢å¯è§†åŒ–"""
    try:
        # éªŒè¯å¯¹è¯æƒé™
        conversation = Conversation.query.filter_by(
            id=conversation_id,
            user_id=current_user.id
        ).first()
        
        if not conversation:
            return jsonify({
                'success': False,
                'error': 'å¯¹è¯ä¸å­˜åœ¨æˆ–æ— æƒé™è®¿é—®'
            }), 404
        
        # è·å–å¯¹è¯è¯¦ç»†ä¿¡æ¯
        conversation_info = {
            'id': conversation.id,
            'title': conversation.title,
            'user_id': conversation.user_id,
            'knowledge_base_id': conversation.knowledge_base_id,
            'model_name': conversation.model_name,
            'system_prompt': conversation.system_prompt,
            'temperature': conversation.temperature,
            'max_tokens': conversation.max_tokens,
            'message_count': conversation.message_count,
            'total_tokens': conversation.total_tokens,
            'is_active': conversation.is_active,
            'created_at': conversation.created_at.isoformat() if conversation.created_at else None,
            'updated_at': conversation.updated_at.isoformat() if conversation.updated_at else None,
            'created_at_formatted': conversation.created_at.strftime('%Y-%m-%d %H:%M:%S') if conversation.created_at else None,
            'updated_at_formatted': conversation.updated_at.strftime('%Y-%m-%d %H:%M:%S') if conversation.updated_at else None
        }
        
        # è·å–æ‰€æœ‰æ¶ˆæ¯çš„è¯¦ç»†ä¿¡æ¯
        messages = Message.query.filter_by(conversation_id=conversation_id)\
            .order_by(Message.created_at.asc()).all()
        
        messages_info = []
        for i, msg in enumerate(messages):
            msg_info = {
                'sequence': i + 1,
                'id': msg.id,
                'conversation_id': msg.conversation_id,
                'role': msg.role,
                'content': msg.content,
                'content_length': len(msg.content),
                'content_preview': msg.content[:100] + '...' if len(msg.content) > 100 else msg.content,
                'token_count': msg.token_count,
                'used_knowledge_base': msg.used_knowledge_base,
                'relevant_chunks': msg.relevant_chunks,
                'msg_metadata': msg.msg_metadata,
                'created_at': msg.created_at.isoformat() if msg.created_at else None,
                'created_at_formatted': msg.created_at.strftime('%Y-%m-%d %H:%M:%S.%f')[:-3] if msg.created_at else None,
                'updated_at': None,
                'updated_at_formatted': None,
            }
            
            # è®¡ç®—ä¸å‰ä¸€æ¡æ¶ˆæ¯çš„æ—¶é—´å·®
            if i > 0:
                prev_msg = messages[i-1]
                if msg.created_at and prev_msg.created_at:
                    time_diff = (msg.created_at - prev_msg.created_at).total_seconds()
                    msg_info['time_diff_seconds'] = round(time_diff, 3)
                    msg_info['time_diff_formatted'] = f"{time_diff:.3f}s"
            
            messages_info.append(msg_info)
        
        # ç»Ÿè®¡ä¿¡æ¯
        user_messages = [m for m in messages if m.role == 'user']
        ai_messages = [m for m in messages if m.role == 'assistant']
        system_messages = [m for m in messages if m.role == 'system']
        
        statistics = {
            'total_messages': len(messages),
            'user_messages': len(user_messages),
            'ai_messages': len(ai_messages),
            'system_messages': len(system_messages),
            'total_characters': sum(len(m.content) for m in messages),
            'total_tokens': sum(m.token_count or 0 for m in messages),
            'avg_message_length': round(sum(len(m.content) for m in messages) / len(messages), 2) if messages else 0,
            'conversation_duration': None,
            'conversation_duration_formatted': None
        }
        
        # è®¡ç®—å¯¹è¯æŒç»­æ—¶é—´
        if messages and len(messages) > 1:
            first_msg = messages[0]
            last_msg = messages[-1]
            if first_msg.created_at and last_msg.created_at:
                duration = (last_msg.created_at - first_msg.created_at).total_seconds()
                statistics['conversation_duration'] = duration
                if duration < 60:
                    statistics['conversation_duration_formatted'] = f"{duration:.1f}ç§’"
                elif duration < 3600:
                    statistics['conversation_duration_formatted'] = f"{duration/60:.1f}åˆ†é’Ÿ"
                else:
                    statistics['conversation_duration_formatted'] = f"{duration/3600:.1f}å°æ—¶"
        
        # æ•°æ®åº“è¡¨ä¿¡æ¯
        table_info = {
            'conversation_table': 'conversations',
            'message_table': 'messages',
            'user_table': 'users',
            'indexes': [
                'conversations.user_id',
                'conversations.created_at',
                'messages.conversation_id',
                'messages.created_at',
                'messages.role'
            ]
        }
        
        # è·å–ç”¨æˆ·ä¿¡æ¯
        user_info = {
            'id': current_user.id,
            'username': current_user.username,
            'email': current_user.email,
            'created_at': current_user.created_at.isoformat() if current_user.created_at else None,
            'created_at_formatted': current_user.created_at.strftime('%Y-%m-%d %H:%M:%S') if current_user.created_at else None
        }
        
        # æŸ¥è¯¢ç›¸å…³ç»Ÿè®¡
        query_stats = {
            'user_total_conversations': Conversation.query.filter_by(
                user_id=current_user.id,
                is_active=True
            ).count(),
            'user_total_messages': db.session.query(Message)\
                .join(Conversation)\
                .filter(Conversation.user_id == current_user.id)\
                .count(),
            'database_total_conversations': Conversation.query.filter_by(is_active=True).count(),
            'database_total_messages': Message.query.count(),
            'database_total_users': db.session.query(db.func.count(db.distinct(Conversation.user_id))).scalar()
        }
        
        return jsonify({
            'success': True,
            'conversation': conversation_info,
            'messages': messages_info,
            'statistics': statistics,
            'table_info': table_info,
            'user_info': user_info,
            'query_stats': query_stats,
            'retrieved_at': datetime.utcnow().isoformat(),
            'retrieved_at_formatted': datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S')
        })
        
    except Exception as e:
        logger.error(f"è·å–å¯¹è¯æ•°æ®åº“ä¿¡æ¯å¤±è´¥: {str(e)}")
        return jsonify({
            'success': False,
            'error': str(e),
            'traceback': traceback.format_exc()
        }), 500

@api_bp.route('/database_search', methods=['GET'])
@login_required
def database_search():
    """æ•°æ®åº“æœç´¢åŠŸèƒ½ - æ”¯æŒå¯¹è¯IDå’Œå…³é”®è¯æœç´¢"""
    try:
        # è·å–æœç´¢å‚æ•°
        search_query = request.args.get('q', '').strip()
        search_type = request.args.get('type', 'keyword')  # keyword, conversation_id, all
        page = request.args.get('page', 1, type=int)
        per_page = min(request.args.get('per_page', 20, type=int), 100)
        
        if not search_query:
            return jsonify({
                'success': False,
                'error': 'æœç´¢å…³é”®è¯ä¸èƒ½ä¸ºç©º'
            }), 400
        
        results = {
            'query': search_query,
            'search_type': search_type,
            'conversations': [],
            'messages': [],
            'total_conversations': 0,
            'total_messages': 0,
            'page': page,
            'per_page': per_page
        }
        
        # åŸºç¡€æŸ¥è¯¢ï¼šåªæ˜¾ç¤ºå½“å‰ç”¨æˆ·çš„æ•°æ®
        base_conversation_query = Conversation.query.filter_by(
            user_id=current_user.id,
            is_active=True
        )
        
        base_message_query = db.session.query(Message)\
            .join(Conversation)\
            .filter(Conversation.user_id == current_user.id)
        
        if search_type == 'conversation_id' or search_type == 'all':
            # æœç´¢å¯¹è¯IDï¼ˆæ”¯æŒéƒ¨åˆ†åŒ¹é…ï¼‰
            conversation_results = base_conversation_query\
                .filter(Conversation.id.contains(search_query))\
                .order_by(Conversation.updated_at.desc())\
                .paginate(page=page, per_page=per_page, error_out=False)
            
            for conv in conversation_results.items:
                # è·å–è¯¥å¯¹è¯çš„æ¶ˆæ¯
                conv_messages = Message.query.filter_by(conversation_id=conv.id)\
                    .order_by(Message.created_at.asc()).all()
                
                results['conversations'].append({
                    'id': conv.id,
                    'title': conv.title,
                    'message_count': len(conv_messages),
                    'created_at_formatted': conv.created_at.strftime('%Y-%m-%d %H:%M:%S') if conv.created_at else None,
                    'updated_at_formatted': conv.updated_at.strftime('%Y-%m-%d %H:%M:%S') if conv.updated_at else None,
                    'messages': [{
                        'id': msg.id,
                        'role': msg.role,
                        'content': msg.content,
                        'content_length': len(msg.content),
                        'created_at_formatted': msg.created_at.strftime('%Y-%m-%d %H:%M:%S.%f')[:-3] if msg.created_at else None,
                        'token_count': msg.token_count
                    } for msg in conv_messages]
                })
            
            results['total_conversations'] = conversation_results.total
        
        if search_type == 'keyword' or search_type == 'all':
            # å…³é”®è¯æœç´¢
            keyword_conversations = base_conversation_query\
                .filter(db.or_(
                    Conversation.title.contains(search_query),
                    Conversation.system_prompt.contains(search_query)
                ))\
                .order_by(Conversation.updated_at.desc())\
                .all()
            
            # æœç´¢æ¶ˆæ¯å†…å®¹
            keyword_messages = base_message_query\
                .filter(Message.content.contains(search_query))\
                .order_by(Message.created_at.desc())\
                .limit(per_page * 3)\
                .all()  # é™åˆ¶æ¶ˆæ¯ç»“æœæ•°é‡
            
            # å¤„ç†å¯¹è¯æœç´¢ç»“æœ
            for conv in keyword_conversations:
                if not any(c['id'] == conv.id for c in results['conversations']):
                    # è·å–åŒ¹é…çš„æ¶ˆæ¯
                    matching_messages = [msg for msg in keyword_messages if msg.conversation_id == conv.id]
                    
                    results['conversations'].append({
                        'id': conv.id,
                        'title': conv.title,
                        'match_type': 'æ ‡é¢˜åŒ¹é…' if search_query in conv.title else 'ç³»ç»Ÿæç¤ºè¯åŒ¹é…',
                        'message_count': conv.message_count,
                        'created_at_formatted': conv.created_at.strftime('%Y-%m-%d %H:%M:%S') if conv.created_at else None,
                        'updated_at_formatted': conv.updated_at.strftime('%Y-%m-%d %H:%M:%S') if conv.updated_at else None,
                        'matching_messages_count': len(matching_messages)
                    })
            
            # å¤„ç†æ¶ˆæ¯æœç´¢ç»“æœ
            message_groups = {}
            for msg in keyword_messages:
                conv_id = msg.conversation_id
                if conv_id not in message_groups:
                    conversation = Conversation.query.get(conv_id)
                    message_groups[conv_id] = {
                        'conversation_id': conv_id,
                        'conversation_title': conversation.title if conversation else 'æœªçŸ¥å¯¹è¯',
                        'messages': []
                    }
                
                message_groups[conv_id]['messages'].append({
                    'id': msg.id,
                    'role': msg.role,
                    'content': msg.content,
                    'content_length': len(msg.content),
                    'content_preview': msg.content[:200] + '...' if len(msg.content) > 200 else msg.content,
                    'created_at_formatted': msg.created_at.strftime('%Y-%m-%d %H:%M:%S.%f')[:-3] if msg.created_at else None,
                    'token_count': msg.token_count,
                    'highlight_query': search_query  # ç”¨äºå‰ç«¯é«˜äº®
                })
            
            results['messages'] = list(message_groups.values())
            results['total_messages'] = len(keyword_messages)
        
        # æœç´¢ç»Ÿè®¡
        results['search_stats'] = {
            'conversations_found': len(results['conversations']),
            'message_groups_found': len(results['messages']),
            'total_message_matches': sum(len(group['messages']) for group in results['messages']),
            'search_time': datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S')
        }
        
        return jsonify({
            'success': True,
            'results': results
        })
        
    except Exception as e:
        logger.error(f"æ•°æ®åº“æœç´¢å¤±è´¥: {str(e)}")
        return jsonify({
            'success': False,
            'error': str(e),
            'traceback': traceback.format_exc()
        }), 500 